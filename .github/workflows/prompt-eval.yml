name: 'Prompt Evaluation'
on:
  pull_request:
    paths:
      - 'prompts/*.txt'
      - 'promptfooconfig.yaml'
      - '.github/workflows/*.yml'
  push:
    branches:
      - main
    paths:
      - 'prompts/*.txt'
      - 'promptfooconfig.yaml'
      - '.github/workflows/*.yml'
  workflow_dispatch:

jobs:
  evaluate:
    runs-on: ubuntu-latest
    environment: promptfoo
    permissions:
      pull-requests: write
      contents: read
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18.17.1'
          # Use a specific version known to work with promptfoo

      - name: Set up promptfoo cache
        uses: actions/cache@v4
        with:
          path: ~/.cache/promptfoo
          key: ${{ runner.os }}-promptfoo-v1
          restore-keys: |
            ${{ runner.os }}-promptfoo-

      - name: Install promptfoo
        run: |
          npm install -g promptfoo@0.28.0
          echo "Installed promptfoo version:"
          promptfoo --version

      - name: Run promptfoo evaluation (PR)
        if: github.event_name == 'pull_request'
        uses: promptfoo/promptfoo-action@v1
        with:
          openai-api-key: ${{ secrets.OPENAI_API_KEY }}
          github-token: ${{ secrets.GITHUB_TOKEN }}
          prompts: 'prompts/*.txt'
          config: 'promptfooconfig.yaml'
          cache-path: ~/.cache/promptfoo
          no-cache: true
 
      - name: Verify API Keys
        run: |
          if [[ -z "${{ secrets.OPENAI_API_KEY }}" ]]; then
            echo "::error::OPENAI_API_KEY is not set in repository secrets"
            exit 1
          else
            echo "âœ… OPENAI_API_KEY is set"
          fi

      - name: Generate HTML report
        if: github.event_name == 'push' || github.event_name == 'workflow_dispatch'
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          # Create output directory
          mkdir -p promptfoo-results
          
          # Create a minimal test file to verify setup
          echo "Test prompt" > test-prompt.txt
          
          # Create minimal config file
          cat > test-config.yaml << EOL
          prompts: 
            - test-prompt.txt
          providers:
            - openai:gpt-3.5-turbo
          EOL
          
          # Run with minimal config first
          echo "Testing with minimal configuration..."
          promptfoo eval -c test-config.yaml --no-cache
          
          # If that works, run the actual evaluation
          echo "Running actual evaluation..."
          promptfoo eval -c promptfooconfig.yaml --no-cache --output promptfoo-results/report.html
 
      - name: Upload evaluation results
        if: github.event_name == 'push' || github.event_name == 'workflow_dispatch'
        uses: actions/upload-artifact@v4
        with:
          name: promptfoo-results-${{ github.sha }}
          path: |
            promptfoo-results/
          retention-days: 30
          if-no-files-found: warn